# -*- coding: utf-8 -*-
"""text_classification_bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AXv0nvcWh1Ckpsy5PxAo4snKNzjOzbQq
"""

!pip install torch transformers

import torch
from transformers import BertForSequenceClassification, BertTokenizer
from typing import List

model_name = "bert-base-uncased"
model = BertForSequenceClassification.from_pretrained(model_name)
tokenizer = BertTokenizer.from_pretrained(model_name)

categories = ["Travel", "Sports", "Technology", "Politics", "Weather"]

dataset = [
    ("Ukraine, a country in Eastern Europe, has a rich cultural heritage and a diverse landscape.", "Travel"),
    ("The situation in Ukraine has been a topic of international concern, with ongoing political and territorial disputes.", "Politics"),
    ("Ukraine boasts a vibrant cuisine, known for dishes like borscht and pierogi.", "Travel"),
    ("Football, also known as soccer in some parts of the world, is a globally beloved sport with a massive following.", "Sports"),
    ("The excitement of football matches often transcends language and cultural barriers, bringing people together.", "Travel"),
    ("Football stars like Lionel Messi and Cristiano Ronaldo are icons of the sport, celebrated for their incredible skills.", "Sports"),
    ("Theano is a deep learning framework, widely recognized for its contributions to machine learning and artificial intelligence.", "Technology"),
    ("Andrew NG is a renowned figure in the field of artificial intelligence and co-founder of Google Brain.", "Technology"),
    ("Andrew NG's online courses on platforms like Coursera have educated thousands of aspiring AI practitioners.", "Technology"),
    ("Winter, with its frosty landscapes and cozy evenings by the fireplace, is a season cherished by many.", "Weather"),
    ("Israel was let down by its extensive array of electronic sensors, surveillance systems, and old-fashioned human intelligence in the form of agents on the ground.", "Politics"),
    ("Two decades of Israeli policy towards the Palestinians have gone up in flames.", "Politics")
]

# tokenize and preprocess dataset
input_texts = [item[0] for item in dataset]
labels = [item[1] for item in dataset]

encoded_texts = tokenizer(input_texts, padding=True, truncation=True, return_tensors='pt')

# convert category labels to numerical values by mapping them to their corresponding index in "categories" list
labels = [categories.index(label) for label in labels]
labels

# extract input IDs and attention masks from encoded texts.
# these are tokenized representations of input texts
# convert labels and other tensors to PyTorch tensors for training

input_ids = encoded_texts["input_ids"]
attention_mask = encoded_texts["attention_mask"]
labels = torch.tensor(labels)

# define BERT model for sequence classification
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(categories))

# define optimizer AdamW to update model parameters during training with a specified learning rate
# define loss function as CrossEntropyLoss which is commonly used for classification tasks
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
loss_fn = torch.nn.CrossEntropyLoss()

batch_size = 2
epochs = 10

# perform training loop, where model is trained on batches of data.
# Optimizer is used to minimize loss updating model's parameters through backpropagation

for epoch in range(epochs):
  for i in range(0, len(input_ids), batch_size):
    batch_input_ids = input_ids[i: i + batch_size]
    batch_attention_mask = attention_mask[i: i + batch_size]
    batch_labels = labels[i: i + batch_size]

    optimizer.zero_grad()

    outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, labels=batch_labels)
    loss = outputs.loss
    loss.backward()
    optimizer.step()

# classification
def classify_with_finetuned_bert(texts: List[str]) -> List[str]:
  encoded_texts = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')
  with torch.no_grad():
    outputs = model(**encoded_texts)
  predicted_labels = [categories[idx] for idx in torch.argmax(outputs.logits, dim=1)]
  return predicted_labels

texts_to_classify = [
    "Ukraine, a country in Eastern Europe, has a rich cultural heritage",
    "Two decades of Israeli policy towards the Palestinians have gone up in flames."
]

predicted_categories = classify_with_finetuned_bert(texts_to_classify)

for text, category in zip(texts_to_classify, predicted_categories):
  print(f"Texts: {text}")
  print(f"Predicted Category: {category}\n")

